# RAG Chat Bot avec Ollama

Un chatbot intelligent qui r√©pond √† vos questions en cherchant dans vos documents, **100% gratuit et local** gr√¢ce √† Ollama.
<img width="1793" height="1787" alt="image" src="https://github.com/user-attachments/assets/2546c539-451d-4d69-a861-366119c264c2" />



##  Fonctionnalit√©s

-  **IA Locale** : Utilise Ollama (Llama 3.2) - aucun co√ªt, aucune limite
-  **Recherche S√©mantique** : Trouve les informations pertinentes dans vos documents
-  **Base Vectorielle** : Qdrant pour une recherche ultra-rapide
-  **Docker** : D√©ploiement en un clic
-  **Priv√©** : Vos donn√©es ne quittent jamais votre machine

##  D√©marrage rapide

### 1. Pr√©requis
- Docker & Docker Compose
- 4GB de RAM minimum (pour les mod√®les Ollama)

### 2. Cloner le projet
```bash
git clone https://github.com/votre-username/poc-node-et-ia-rag.git
cd poc-node-et-ia-rag
```

### 3. Lancer l'application
```bash
docker compose up --build -d
```

**Premi√®re fois** : Ollama va t√©l√©charger les mod√®les (~2.3GB). Cela peut prendre 5-10 minutes.

### 4. T√©l√©charger les mod√®les IA
```bash
docker compose exec ollama ollama pull nomic-embed-text
docker compose exec ollama ollama pull llama3.2
```

### 5. Indexer les documents
```bash
docker compose exec nodeapp npm run index
```

### 6. Utiliser l'application
- **Chat** : http://localhost:3000
- **Qdrant Dashboard** : http://localhost:6333/dashboard

##  Ajouter des documents au corpus

1. Ajoutez vos fichiers `.json` dans `backend/corpus/`
2. Format requis :
```json
{
  "title": "Titre du document",
  "author": "Auteur",
  "date": "2024-01-01",
  "category": "Cat√©gorie",
  "tags": ["tag1", "tag2"],
  "text": "Contenu complet du document..."
}
```
3. Relancez l'indexer :
```bash
docker compose exec nodeapp npm run index
```

## üõ†Ô∏è Stack technique

- **Backend** : Node.js + Express
- **IA Embeddings** : Ollama (nomic-embed-text, 768 dimensions)
- **IA Chat** : Ollama (Llama 3.2)
- **Base Vectorielle** : Qdrant
- **Conteneurisation** : Docker

##  Architecture

```
Question utilisateur
    ‚Üì
G√©n√©ration d'embedding (Ollama)
    ‚Üì
Recherche vectorielle (Qdrant)
    ‚Üì
R√©cup√©ration du contexte pertinent
    ‚Üì
G√©n√©ration de r√©ponse (Llama 3.2)
    ‚Üì
Affichage avec sources
```

##  Comparaison avec OpenAI

| Crit√®re | OpenAI | Ollama (ce projet) |
|---------|--------|-------------------|
| Co√ªt | ~0.002$/1K tokens | **Gratuit** |
| Vitesse | Rapide | Rapide (apr√®s t√©l√©chargement) |
| Privacit√© | Donn√©es envoy√©es | **100% local** |
| Limites | Rate limits | **Aucune** |
| Connexion | Internet requis | **Fonctionne hors ligne** |

##  Commandes utiles

```bash
# Voir les logs
docker compose logs -f nodeapp

# Red√©marrer l'application
docker compose restart nodeapp

# Arr√™ter tout
docker compose down

# Supprimer les volumes (r√©initialisation compl√®te)
docker compose down -v

# Lister les mod√®les Ollama install√©s
docker compose exec ollama ollama list

# T√©l√©charger un nouveau mod√®le
docker compose exec ollama ollama pull <model-name>
```

##  D√©pannage

### L'application ne trouve pas de r√©sultats
- V√©rifiez que les documents sont bien index√©s : `docker compose exec nodeapp npm run index`
- Assurez-vous que votre question est en rapport avec le contenu des documents

### Ollama est lent
- Premi√®re utilisation : les mod√®les se t√©l√©chargent (~2.3GB)
- Assurez-vous d'avoir au moins 4GB de RAM disponible

### Erreur "Cannot connect to Ollama"
- V√©rifiez que le conteneur Ollama est d√©marr√© : `docker compose ps`
- Red√©marrez : `docker compose restart ollama`

##  Licence

MIT - Voir [LICENSE](LICENSE)

##  Cr√©dits

- [Ollama](https://ollama.ai/) - IA locale
- [Qdrant](https://qdrant.tech/) - Base vectorielle
- [Node.js](https://nodejs.org/) - Runtime JavaScript
