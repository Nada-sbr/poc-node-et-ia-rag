# ğŸ§  RAG Chat Bot avec Ollama

Un chatbot intelligent qui rÃ©pond Ã  vos questions en cherchant dans vos documents, **100% gratuit et local** grÃ¢ce Ã  Ollama.

![RAG Chat Bot Demo](./backend/public/images/demo-screenshot.jpeg)

## âœ¨ FonctionnalitÃ©s

- ğŸ¤– **IA Locale** : Utilise Ollama (Llama 3.2) - aucun coÃ»t, aucune limite
- ğŸ” **Recherche SÃ©mantique** : Trouve les informations pertinentes dans vos documents
- ğŸ“š **Base Vectorielle** : Qdrant pour une recherche ultra-rapide
- ğŸ³ **Docker** : DÃ©ploiement en un clic
- ğŸ”’ **PrivÃ©** : Vos donnÃ©es ne quittent jamais votre machine

## ğŸš€ DÃ©marrage rapide

### 1. PrÃ©requis
- Docker & Docker Compose
- 4GB de RAM minimum (pour les modÃ¨les Ollama)

### 2. Cloner le projet
```bash
git clone https://github.com/votre-username/poc-node-et-ia-rag.git
cd poc-node-et-ia-rag
```

### 3. Lancer l'application
```bash
docker compose up --build -d
```

**PremiÃ¨re fois** : Ollama va tÃ©lÃ©charger les modÃ¨les (~2.3GB). Cela peut prendre 5-10 minutes.

### 4. TÃ©lÃ©charger les modÃ¨les IA
```bash
docker compose exec ollama ollama pull nomic-embed-text
docker compose exec ollama ollama pull llama3.2
```

### 5. Indexer les documents
```bash
docker compose exec nodeapp npm run index
```

### 6. Utiliser l'application
- **Chat** : http://localhost:3000
- **Qdrant Dashboard** : http://localhost:6333/dashboard

## ğŸ“‚ Ajouter des documents au corpus

1. Ajoutez vos fichiers `.json` dans `backend/corpus/`
2. Format requis :
```json
{
  "title": "Titre du document",
  "author": "Auteur",
  "date": "2024-01-01",
  "category": "CatÃ©gorie",
  "tags": ["tag1", "tag2"],
  "text": "Contenu complet du document..."
}
```
3. Relancez l'indexer :
```bash
docker compose exec nodeapp npm run index
```

## ğŸ› ï¸ Stack technique

- **Backend** : Node.js + Express
- **IA Embeddings** : Ollama (nomic-embed-text, 768 dimensions)
- **IA Chat** : Ollama (Llama 3.2)
- **Base Vectorielle** : Qdrant
- **Conteneurisation** : Docker

## ğŸ¯ Architecture

```
Question utilisateur
    â†“
GÃ©nÃ©ration d'embedding (Ollama)
    â†“
Recherche vectorielle (Qdrant)
    â†“
RÃ©cupÃ©ration du contexte pertinent
    â†“
GÃ©nÃ©ration de rÃ©ponse (Llama 3.2)
    â†“
Affichage avec sources
```

## ğŸ“Š Comparaison avec OpenAI

| CritÃ¨re | OpenAI | Ollama (ce projet) |
|---------|--------|-------------------|
| CoÃ»t | ~0.002$/1K tokens | **Gratuit** |
| Vitesse | Rapide | Rapide (aprÃ¨s tÃ©lÃ©chargement) |
| PrivacitÃ© | DonnÃ©es envoyÃ©es | **100% local** |
| Limites | Rate limits | **Aucune** |
| Connexion | Internet requis | **Fonctionne hors ligne** |

## ğŸ”§ Commandes utiles

```bash
# Voir les logs
docker compose logs -f nodeapp

# RedÃ©marrer l'application
docker compose restart nodeapp

# ArrÃªter tout
docker compose down

# Supprimer les volumes (rÃ©initialisation complÃ¨te)
docker compose down -v

# Lister les modÃ¨les Ollama installÃ©s
docker compose exec ollama ollama list

# TÃ©lÃ©charger un nouveau modÃ¨le
docker compose exec ollama ollama pull <model-name>
```

## ğŸ› DÃ©pannage

### L'application ne trouve pas de rÃ©sultats
- VÃ©rifiez que les documents sont bien indexÃ©s : `docker compose exec nodeapp npm run index`
- Assurez-vous que votre question est en rapport avec le contenu des documents

### Ollama est lent
- PremiÃ¨re utilisation : les modÃ¨les se tÃ©lÃ©chargent (~2.3GB)
- Assurez-vous d'avoir au moins 4GB de RAM disponible

### Erreur "Cannot connect to Ollama"
- VÃ©rifiez que le conteneur Ollama est dÃ©marrÃ© : `docker compose ps`
- RedÃ©marrez : `docker compose restart ollama`

## ğŸ“ Licence

MIT - Voir [LICENSE](LICENSE)

## ğŸ™ CrÃ©dits

- [Ollama](https://ollama.ai/) - IA locale
- [Qdrant](https://qdrant.tech/) - Base vectorielle
- [Node.js](https://nodejs.org/) - Runtime JavaScript
